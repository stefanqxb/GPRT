\documentclass{bioinfo}
\copyrightyear{2005}
\pubyear{2005}

\begin{document}
\firstpage{1}

\title[short Title]{This is a title}
\author[Sample \textit{et~al}]{Corresponding Author\,$^{1,*}$, Co-Author\,$^{2}$ and Co-Author\,$^2$\footnote{to whom correspondence should be addressed}}
\address{$^{1}$Department of XXXXXXX, Address XXXX etc.\\
$^{2}$Department of XXXXXXXX, Address XXXX etc.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\maketitle

\begin{abstract}

\section{Motivation:}
Text Text Text  Text Text Text Text Text Text Text Text
Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text.

\section{Results:}
Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text

\section{Availability:}
Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text

\section{Contact:} \href{name@bio.com}{name@bio.com}
\end{abstract}

\section{Introduction}

* Section describing the RT prediction (citing some relevant papers)

The role of a machine learning framework is to learn the association between the inputs and the outputs in a training set. After this association is learnt, the model is used to predict the output of the unseen inputs. When addressing regression problems, a feature that is usually neglected by most frameworks is the certainty of the prediction. While most frameworks can predict an output for a given input, they do not provide the user with notion of how certain the framework is regarding this prediction. Inclusion of such notion can highly benefit many machine tasks by removing false positives that the method is uncertain about the decisions it has made. To address this problem, we will employ Gaussian Process (GP) framework \cite{Rasmussen:2006vza} to predict peptide retention time. We will show, how the property in assigning certainty to the predictions provide a ground for removing more faulty predictions. 
Moreover, we will argue how this framework can provide us with a model with significantly higher performance compared to previously used frameworks for solving the retention time prediction task.

\section{Approach}
The task of retention time prediction, focuses on determining the retention time of a peptide given its amino acid sequence. To obtain a robust model for solving this task, one needs to address the following problems. Most machine learning frameworks require the inputs to come from a vector space. To map the peptide into a vector space by extracting biologically meaningful features \cite{elute}, collecting general statistical entities collected from the sequences \cite{Rieck:2011ed} or using different kernels such as spectrum kernel \cite{Leslie:2002tx} or string kernel \cite{Lodhi:2002ts}. Once the feature vectors are calculated, a machine learning framework should be selected for solving the prediction problem. The choice this method can highly affect the quality of the prediction. For example in \cite{elute}, the authors have chosen Support Vector Regression (SVR) framework \cite{Bishop:2006ui} to solve the regression problem.

In this paper, we throughly analyze the retention time prediction problem from the machine learning perspective. First, we will look at different methods that can be used for mapping the peptide sequences into vector spaces. In this process, our aim is to determine the pros and cons of different feature extraction techniques in association with large data analysis. Second, we will look at Gaussian Process (GP) \cite{Rasmussen:2006vza} as more sophisticated framework for solving the prediction task and compare its performance with the widely used SVR framework. We will also analyze how varying the size of the training set can affect the performance of both models.
\begin{methods}
\section{Methods}


\section{Results}

- GPs are performing better than epsilon-SVRs (and RVMs)
* Plot of deltaRT95 as a function of training set size

-The Elude features give better performance (, but take longer time to calculate)
* Compare with Elude-RBF, BOW-RBF (1,2 and 3-mers), and spectrum kernel
* Plot of deltaRT95 as a function of training set size

- GPs are predicting confidence
* Plot a actual obs-pred RT error as a function of stdv.

- Our Method ... outperforms the state-of-the-art, (Elude and SSRC) 
* Plot observed vs predicted RT in colormaps in 3 different subplots
* Alternatively (and probably better), plot 3 histograms of obs-pred RT for the three methods, and overlay them using transparent colors (e.g. set their alpha-value)

- Our method can predict RT of PTMs
* Plot histogram of error (obs-pred RT) for a set of phospho peptides. 

- GP confidence, observed vs predicted
* Compare the test and train set

- GP confidence, Find a list of peptides and sort them according to their similarity to the training set
* Plot their confidence 



\begin{table}[!t]
\processtable{This is table caption\label{Tab:01}}
{\begin{tabular}{llll}\toprule
head1 & head2 & head3 & head4\\\midrule
row1 & row1 & row1 & row1\\
row2 & row2 & row2 & row2\\
row3 & row3 & row3 & row3\\
row4 & row4 & row4 & row4\\\botrule
\end{tabular}}{This is a footnote}
\end{table}

\end{methods}

\begin{figure}[!tpb]%figure1
%\centerline{\includegraphics{fig01.eps}}
\caption{Caption, caption.}\label{fig:01}
\end{figure}

\begin{figure}[!tpb]%figure2
%\centerline{\includegraphics{fig02.eps}}
\caption{Caption, caption.}\label{fig:02}
\end{figure}

\section{Discussion}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     please remove the " % " symbol from \centerline{\includegraphics{fig01.eps}}
%     as it may ignore the figures.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Conclusion}



\begin{enumerate}
\item this is item, use enumerate
\item this is item, use enumerate
\item this is item, use enumerate
\end{enumerate}

Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text. Figure \ref{fig:02} shows that the above method  Text Text Text Text  Text Text Text Text Text Text  Text Text.  \citealp{Boffelli03} might want to know about  text text text text
Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text. Figure \ref{fig:02} shows that the above method  Text Text Text Text  Text Text Text Text Text Text  Text Text.  \citealp{Boffelli03} might want to know about  text text text text
Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text.






Text Text Text Text Text Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text Text Text. Figure \ref{fig:02} shows that the above method  Text Text Text Text


\section*{Acknowledgement}
Text Text Text Text Text Text  Text Text.  \citealp{Boffelli03} might want to know about  text text text text

\paragraph{Funding\textcolon} Text Text Text Text Text Text  Text Text.

%\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
%\bibliographystyle{bioinformatics}
%
%\bibliographystyle{plain}
%
%\bibliography{Document}


\begin{thebibliography}{}
\bibitem[Bofelli {\it et~al}., 2000]{Boffelli03} Bofelli,F., Name2, Name3 (2003) Article title, {\it Journal Name}, {\bf 199}, 133-154.

\bibitem[Bag {\it et~al}., 2001]{Bag01} Bag,M., Name2, Name3 (2001) Article title, {\it Journal Name}, {\bf 99}, 33-54.

\bibitem[Yoo \textit{et~al}., 2003]{Yoo03}
Yoo,M.S. \textit{et~al}. (2003) Oxidative stress regulated genes
in nigral dopaminergic neurnol cell: correlation with the known
pathology in Parkinson's disease. \textit{Brain Res. Mol. Brain
Res.}, \textbf{110}(Suppl. 1), 76--84.

\bibitem[Lehmann, 1986]{Leh86}
Lehmann,E.L. (1986) Chapter title. \textit{Book Title}. Vol.~1, 2nd edn. Springer-Verlag, New York.

\bibitem[Crenshaw and Jones, 2003]{Cre03}
Crenshaw, B.,III, and Jones, W.B.,Jr (2003) The future of clinical
cancer management: one tumor, one chip. \textit{Bioinformatics},
doi:10.1093/bioinformatics/btn000.

\bibitem[Auhtor \textit{et~al}. (2000)]{Aut00}
Auhtor,A.B. \textit{et~al}. (2000) Chapter title. In Smith, A.C.
(ed.), \textit{Book Title}, 2nd edn. Publisher, Location, Vol. 1, pp.
???--???.

\bibitem[Bardet, 1920]{Bar20}
Bardet, G. (1920) Sur un syndrome d'obesite infantile avec
polydactylie et retinite pigmentaire (contribution a l'etude des
formes cliniques de l'obesite hypophysaire). PhD Thesis, name of
institution, Paris, France.

\end{thebibliography}
\end{document}
